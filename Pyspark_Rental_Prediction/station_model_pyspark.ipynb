{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6fb93b2-a3dd-42e6-a12d-ede29795a0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ml-cluster-less-wrkrs-m.us-central1-b.c.welu420.internal:37777\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289e1847-f23a-4c87-aa04-df1f91224d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ml-cluster-less-wrkrs-m.us-central1-b.c.welu420.internal:37777\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1cdc3ca9a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b74056b9-a637-4409-acb4-3043cbad04bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, date_format,\n",
    "    window, date_sub, hour,\n",
    "    date_trunc, mean, month,\n",
    "    next_day, to_timestamp, weekofyear,\n",
    "    window, year, dayofweek\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f877e099-4cb6-4fa4-8a8b-28db3f67327d",
   "metadata": {},
   "source": [
    "# Notebook for training ml models for stations and write back the predictions in a BigQuery table\n",
    "---\n",
    "### Reading in the complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adf6d22f-8003-439a-b2e7-87d379339e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_df = spark.read.format('bigquery').option(\n",
    "    'project', 'welu420'\n",
    ").option('table', 'Bikesharing.System_Data').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3c283df-997c-40d1-880b-b5e8dab554d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+-------------------+--------------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+-------+----+------------+-------------+----+----+-----------+-------+\n",
      "|Bike_number|Duration|         Start_date|           End_date|       Start_station|Start_station_number|Start_capacity|         End_station|End_station_number|End_capacity|       start_lat|        start_lon|         end_lat|          end_lon|Member_type|Holiday|temp|rel_humidity|precipitation|snow|wspd|sun_minutes|ride_id|\n",
      "+-----------+--------+-------------------+-------------------+--------------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+-------+----+------------+-------------+----+----+-----------+-------+\n",
      "|     W00939|     376|2010-10-03 09:21:02|2010-10-03 09:27:18|Calvert & Biltmor...|               31106|            23|16th & Harvard St NW|             31103|          19|38.9232098562661|-77.0476407613226|38.9261054903053|-77.0366523027635|     Member|   null|11.7|        83.0|          0.0|null| 9.4|       null|   5499|\n",
      "|     W00221|    6365|2010-10-03 13:49:04|2010-10-03 15:35:09|Calvert & Biltmor...|               31106|            23|Calvert & Biltmor...|             31106|          23|38.9232098562661|-77.0476407613226|38.9232098562661|-77.0476407613226|     Casual|   null|14.4|        78.0|          0.0|null|20.5|       null|   5702|\n",
      "|     W00711|     239|2010-10-07 20:50:49|2010-10-07 20:54:48|Crystal City Metr...|               31007|            19|Aurora Hills Cmty...|             31004|          12|38.8577893016538| -77.051698705356|38.8578756772498|-77.0594872301002|     Member|   null|24.4|        35.0|          0.0|null|20.5|       null|   8313|\n",
      "+-----------+--------+-------------------+-------------------+--------------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+-------+----+------------+-------------+----+----+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bike_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73509484-8940-4523-bd5d-9b4b177c6d71",
   "metadata": {},
   "source": [
    "#### Code for the development purpose. Not to use in the production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465bc231-157f-4990-8128-425060077ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to get the dataframe for a particular station\n",
    "# union_station = bike_df.filter((bike_df.Start_station == 'Columbus Circle / Union Station') | \n",
    "#                                (bike_df.End_station == 'Columbus Circle / Union Station'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b12203a-a6a4-44f7-a2fa-f0ab3892e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del bike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1030ee55-21bf-4fda-9f62-d9af37adabd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bike_number</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Start_date</th>\n",
       "      <th>End_date</th>\n",
       "      <th>Start_station</th>\n",
       "      <th>Start_station_number</th>\n",
       "      <th>Start_capacity</th>\n",
       "      <th>End_station</th>\n",
       "      <th>End_station_number</th>\n",
       "      <th>End_capacity</th>\n",
       "      <th>...</th>\n",
       "      <th>end_lon</th>\n",
       "      <th>Member_type</th>\n",
       "      <th>Holiday</th>\n",
       "      <th>temp</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snow</th>\n",
       "      <th>wspd</th>\n",
       "      <th>sun_minutes</th>\n",
       "      <th>ride_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W00721</td>\n",
       "      <td>587</td>\n",
       "      <td>2011-02-27 15:11:50</td>\n",
       "      <td>2011-02-27 15:21:37</td>\n",
       "      <td>13th &amp; D St NE</td>\n",
       "      <td>31622</td>\n",
       "      <td>31</td>\n",
       "      <td>Columbus Circle / Union Station</td>\n",
       "      <td>31623</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.004936</td>\n",
       "      <td>Casual</td>\n",
       "      <td>None</td>\n",
       "      <td>6.1</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>198071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W00779</td>\n",
       "      <td>412</td>\n",
       "      <td>2011-03-22 09:31:08</td>\n",
       "      <td>2011-03-22 09:38:00</td>\n",
       "      <td>13th &amp; D St NE</td>\n",
       "      <td>31622</td>\n",
       "      <td>31</td>\n",
       "      <td>Columbus Circle / Union Station</td>\n",
       "      <td>31623</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.004936</td>\n",
       "      <td>Member</td>\n",
       "      <td>None</td>\n",
       "      <td>11.7</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>244029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>W00416</td>\n",
       "      <td>2811</td>\n",
       "      <td>2011-04-18 15:02:10</td>\n",
       "      <td>2011-04-18 15:49:02</td>\n",
       "      <td>Georgetown Harbor / 30th St NW</td>\n",
       "      <td>31215</td>\n",
       "      <td>19</td>\n",
       "      <td>Columbus Circle / Union Station</td>\n",
       "      <td>31623</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>-77.004936</td>\n",
       "      <td>Casual</td>\n",
       "      <td>None</td>\n",
       "      <td>17.2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>308511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Bike_number  Duration          Start_date            End_date  \\\n",
       "0      W00721       587 2011-02-27 15:11:50 2011-02-27 15:21:37   \n",
       "1      W00779       412 2011-03-22 09:31:08 2011-03-22 09:38:00   \n",
       "2      W00416      2811 2011-04-18 15:02:10 2011-04-18 15:49:02   \n",
       "\n",
       "                    Start_station  Start_station_number  Start_capacity  \\\n",
       "0                  13th & D St NE                 31622              31   \n",
       "1                  13th & D St NE                 31622              31   \n",
       "2  Georgetown Harbor / 30th St NW                 31215              19   \n",
       "\n",
       "                       End_station  End_station_number  End_capacity  ...  \\\n",
       "0  Columbus Circle / Union Station               31623            55  ...   \n",
       "1  Columbus Circle / Union Station               31623            55  ...   \n",
       "2  Columbus Circle / Union Station               31623            55  ...   \n",
       "\n",
       "     end_lon  Member_type  Holiday  temp rel_humidity precipitation  snow  \\\n",
       "0 -77.004936       Casual     None   6.1         79.0           0.0   NaN   \n",
       "1 -77.004936       Member     None  11.7         86.0           0.0   NaN   \n",
       "2 -77.004936       Casual     None  17.2         54.0           0.0   NaN   \n",
       "\n",
       "   wspd  sun_minutes  ride_id  \n",
       "0  11.2          NaN   198071  \n",
       "1  11.2          NaN   244029  \n",
       "2   7.6          NaN   308511  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union_station.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd31e81-75d8-4add-9334-533affc6f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "17.523396611798503"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean temperature for replacing nan values in temperature column\n",
    "# mean_temp = union_station.select(mean('temp')).collect()[0][0]\n",
    "# mean_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8863cc3f-75a5-4464-83d7-036e2b539e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_station = union_station.withColumn('minutes',union_station.Duration/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be651cd3-3942-4344-8e30-a4e3e1d163a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "converting duration in minutes and saving in a new column\n",
    "union_station = union_station.withColumn('minutes',union_station.Duration/60)\n",
    "handling missing values in the data\n",
    "union_station = union_station.fillna(\n",
    "    {\n",
    "        'temp': mean_temp,\n",
    "        'Holiday': 'No-Holiday',\n",
    "        'rel_humidity': 0,\n",
    "        'precipitation': 0,\n",
    "        'snow': 0,\n",
    "        'sun_minutes': 0,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc4927a9-f6b5-496b-a439-4bc9f281dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new columns: hour_of_day, day_of_week, month and year from timestamp\n",
    "union_station = union_station.withColumn('hour_of_day', hour('Start_date').alias('hour_of_day'))\n",
    "union_station = union_station.withColumn('year', year('Start_date').alias('year'))\n",
    "union_station = union_station.withColumn('month', month('Start_date').alias('month'))\n",
    "union_station = union_station.withColumn('day_of_week', dayofweek('Start_date').alias('day_of_week'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6d62f3-f2a6-4e9c-b9d4-9c5e5f30c880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional column: 1 if holiday, 0 otherwise\n",
    "union_station = union_station.withColumn(\n",
    "    'is_holiday',\n",
    "    F.when((F.col(\"Holiday\") == 'No-Holiday'), 0).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab39e42e-c3d1-4287-b4f4-9b97107594c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional column: 1 if weekend, 0 if day of week between 0-5\n",
    "union_station = union_station.withColumn(\n",
    "    'workday',\n",
    "    F.when((F.col('day_of_week') == 5) & (F.col('day_of_week') == 6), 0).otherwise(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba5e488a-a06f-49da-addb-5a08b14dc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_station.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6659d3f-58d1-463d-bdca-f6063a19f8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rides ending at this station\n",
    "# duration is at least 5 minutes\n",
    "incomings = union_station.filter( \\\n",
    "    (union_station['End_station'] == 'Columbus Circle / Union Station') & \\\n",
    "    (union_station['minutes'] >= 5))\n",
    "\n",
    "# rides begining from this station\n",
    "outgoings = union_station.filter( \\\n",
    "    (union_station['Start_station'] == 'Columbus Circle / Union Station') & \\\n",
    "    (union_station['minutes'] >= 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383d71e9-2f1a-47dd-9b00-b0514e6f1053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/02 13:21:12 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------------------+-------------------+--------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+----------+----+------------+-------------+----+----+-----------+-------+-----------------+-----------+----+-----+-----------+----------+-------+\n",
      "|Bike_number|Duration|         Start_date|           End_date| Start_station|Start_station_number|Start_capacity|         End_station|End_station_number|End_capacity|       start_lat|        start_lon|         end_lat|          end_lon|Member_type|   Holiday|temp|rel_humidity|precipitation|snow|wspd|sun_minutes|ride_id|          minutes|hour_of_day|year|month|day_of_week|is_holiday|workday|\n",
      "+-----------+--------+-------------------+-------------------+--------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+----------+----+------------+-------------+----+----+-----------+-------+-----------------+-----------+----+-----+-----------+----------+-------+\n",
      "|     W00721|     587|2011-02-27 15:11:50|2011-02-27 15:21:37|13th & D St NE|               31622|            31|Columbus Circle /...|             31623|          55|38.8948426251047|-76.9876334240151|38.8969691496826|-77.0049361412587|     Casual|No-Holiday| 6.1|        79.0|          0.0| 0.0|11.2|        0.0| 198071|9.783333333333333|         15|2011|    2|          1|         0|      1|\n",
      "|     W00779|     412|2011-03-22 09:31:08|2011-03-22 09:38:00|13th & D St NE|               31622|            31|Columbus Circle /...|             31623|          55|38.8948426251047|-76.9876334240151|38.8969691496826|-77.0049361412587|     Member|No-Holiday|11.7|        86.0|          0.0| 0.0|11.2|        0.0| 244029|6.866666666666666|          9|2011|    3|          3|         0|      1|\n",
      "+-----------+--------+-------------------+-------------------+--------------+--------------------+--------------+--------------------+------------------+------------+----------------+-----------------+----------------+-----------------+-----------+----------+----+------------+-------------+----+----+-----------+-------+-----------------+-----------+----+-----+-----------+----------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "incomings.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "229d9019-f747-4db6-a42d-432fcc8a2cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "incomings.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84043242-e724-4db6-9a7c-f6f4c0dcf883",
   "metadata": {},
   "outputs": [],
   "source": [
    "features: temp, rel_humidity, precipitation, snow, wspd, sun_minutes, is_holiday, workday, month, hour_of_day, year\n",
    "to predict: rentals in hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e7351e6-90a7-4e6f-99d6-3b5f19500958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by every hour to get the total number of rides for every hour\n",
    "# using mean for other features to keep them in the table\n",
    "# mean makes no difference as the values for one hour are always same\n",
    "incomings = (\n",
    "   incomings\n",
    "    .groupBy(window(col(\"Start_date\"), \"1 hour\").alias(\"hour\"))\n",
    "    .agg(\n",
    "        F.count('ride_id').alias('rentals_in_hour'), F.mean('temp').alias('temp'), F.mean('rel_humidity').alias('rel_humidity'),\n",
    "        F.mean('precipitation').alias('precipitation'), F.mean('snow').alias('snow'), F.mean('wspd').alias('wspd'),\n",
    "        F.mean('sun_minutes').alias('sun_minutes'), F.mean('is_holiday').alias('is_holiday'),\n",
    "        F.mean('workday').alias('workday'), F.mean('month').alias('month'), F.mean('hour_of_day').alias('hour_of_day'),\n",
    "        F.mean('year').alias('year')\n",
    "    )\n",
    ")\n",
    "\n",
    "outgoings = (\n",
    "   outgoings\n",
    "    .groupBy(window(col(\"Start_date\"), \"1 hour\").alias(\"hour\"))\n",
    "    .agg(\n",
    "        F.count('ride_id').alias('rentals_in_hour'), F.mean('temp').alias('temp'), F.mean('rel_humidity').alias('rel_humidity'),\n",
    "        F.mean('precipitation').alias('precipitation'), F.mean('snow').alias('snow'), F.mean('wspd').alias('wspd'),\n",
    "        F.mean('sun_minutes').alias('sun_minutes'), F.mean('is_holiday').alias('is_holiday'),\n",
    "        F.mean('workday').alias('workday'), F.mean('month').alias('month'), F.mean('hour_of_day').alias('hour_of_day'),\n",
    "        F.mean('year').alias('year')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ae44599-f53e-476e-a493-8e09509d4a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>rentals_in_hour</th>\n",
       "      <th>temp</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snow</th>\n",
       "      <th>wspd</th>\n",
       "      <th>sun_minutes</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>workday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2012-06-12 07:00:00, 2012-06-12 08:00:00)</td>\n",
       "      <td>11</td>\n",
       "      <td>23.9</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2013-08-26 08:00:00, 2013-08-26 09:00:00)</td>\n",
       "      <td>35</td>\n",
       "      <td>18.9</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2013.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         hour  rentals_in_hour  temp  \\\n",
       "0  (2012-06-12 07:00:00, 2012-06-12 08:00:00)               11  23.9   \n",
       "1  (2013-08-26 08:00:00, 2013-08-26 09:00:00)               35  18.9   \n",
       "\n",
       "   rel_humidity  precipitation  snow  wspd  sun_minutes  is_holiday  workday  \\\n",
       "0          71.0            0.0   0.0  16.6          0.0         0.0      1.0   \n",
       "1          73.0            0.0   0.0  13.0          0.0         0.0      1.0   \n",
       "\n",
       "   month  hour_of_day    year  \n",
       "0    6.0          7.0  2012.0  \n",
       "1    8.0          8.0  2013.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomings.toPandas().head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1ccf504-b11a-4eee-a739-6bc4cc30251c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59131"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incomings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98de74a3-8f96-4046-b050-4e957e93883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81599ad1-d0bd-4707-915a-f0c673943b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "outgoings_train = outgoings.where(F.col('year') < 2018)\n",
    "outgoings_test = outgoings.where(F.col('year') > 2017)\n",
    "incomings_train = incomings.where(F.col('year') < 2018)\n",
    "incomings_test = incomings.where(F.col('year') > 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e045ad6-572d-47d6-9533-292769664ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp rel_humidity precipitation snow wspd sun_minutes is_holiday workday month hour year\n",
    "va = VectorAssembler(inputCols=['temp', 'rel_humidity', 'precipitation', 'snow', 'wspd',\n",
    "                               'sun_minutes', 'is_holiday', 'workday', 'month', 'hour_of_day', 'year'],outputCol='features')\n",
    "outgoings_ml = va.transform(outgoings).select('features','rentals_in_hour')\n",
    "incomings_ml = va.transform(incomings).select('features','rentals_in_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7cb3af-fdbf-4b8e-9d03-6ed78bdb371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "434392b1-3ce2-40c3-8cc0-f3cbc9a70dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our pipeline\n",
    "# stage 1: create feature vector\n",
    "vectorising_stage = VectorAssembler(inputCols=['temp', 'rel_humidity', 'precipitation', 'snow', 'wspd',\n",
    "                                               'sun_minutes', 'is_holiday', 'workday', 'month', 'hour_of_day',\n",
    "                                               'year'], outputCol='features')\n",
    "# stage 2: scale feature vector\n",
    "scaling_stage = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "# stage 3: estimator\n",
    "gbr_stage = GBTRegressor(featuresCol='features_scaled', labelCol='rentals_in_hour')\n",
    "\n",
    "gbr_pipe = Pipeline(stages=[vectorising_stage, scaling_stage, gbr_stage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0630eb4-68be-48b9-87a2-e55b15b5fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoings_model = gbr_pipe.fit(outgoings_train)\n",
    "outgoings_train = outgoings_model.transform(outgoings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d537309-cb99-49c0-973d-c0ac80fa5c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 335:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.87169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "outgoings_test = outgoings_model.transform(outgoings_test)\n",
    "\n",
    "# select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"rentals_in_hour\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(outgoings_test)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45ab3266-82b8-4c39-bdf3-ce950c2f34b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "incomings_model = gbr_pipe.fit(incomings_train)\n",
    "incomings_train = incomings_model.transform(incomings_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a94e02a-f658-4ab3-ae4c-973f89256437",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 652:==================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 6.38498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "incomings_test = incomings_model.transform(incomings_test)\n",
    "# select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"rentals_in_hour\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(incomings_test)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf070408-d220-4a8b-967a-81452f202119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53ba939a-630f-4456-9945-c190b5b652ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>rentals_in_hour</th>\n",
       "      <th>temp</th>\n",
       "      <th>rel_humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>snow</th>\n",
       "      <th>wspd</th>\n",
       "      <th>sun_minutes</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>workday</th>\n",
       "      <th>month</th>\n",
       "      <th>hour_of_day</th>\n",
       "      <th>year</th>\n",
       "      <th>features</th>\n",
       "      <th>features_scaled</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(2018-10-02 18:00:00, 2018-10-02 19:00:00)</td>\n",
       "      <td>23</td>\n",
       "      <td>28.9</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>[28.9, 57.0, 0.0, 0.0, 20.5, 0.0, 0.0, 1.0, 10...</td>\n",
       "      <td>[2.9452533173777047, 2.973778726395736, 0.0, 0...</td>\n",
       "      <td>18.439240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(2018-05-25 08:00:00, 2018-05-25 09:00:00)</td>\n",
       "      <td>37</td>\n",
       "      <td>18.9</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>[18.9, 81.0, 0.0, 0.0, 7.6, 0.0, 0.0, 1.0, 5.0...</td>\n",
       "      <td>[1.9261345224373223, 4.225896084878151, 0.0, 0...</td>\n",
       "      <td>27.576794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(2018-07-20 08:00:00, 2018-07-20 09:00:00)</td>\n",
       "      <td>32</td>\n",
       "      <td>22.2</td>\n",
       "      <td>76.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>(22.2, 76.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 7.0...</td>\n",
       "      <td>(2.2624437247676483, 3.9650383018609814, 0.0, ...</td>\n",
       "      <td>27.576794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(2018-07-14 20:00:00, 2018-07-14 21:00:00)</td>\n",
       "      <td>10</td>\n",
       "      <td>32.8</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>[32.8, 31.0, 0.0, 0.0, 22.300000000000004, 0.0...</td>\n",
       "      <td>[3.3427096474044533, 1.6173182547064529, 0.0, ...</td>\n",
       "      <td>6.295239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2018-10-18 18:00:00, 2018-10-18 19:00:00)</td>\n",
       "      <td>19</td>\n",
       "      <td>14.4</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>[14.4, 36.0, 0.0, 0.0, 24.1, 0.0, 0.0, 1.0, 10...</td>\n",
       "      <td>[1.4675310647141504, 1.8781760377236227, 0.0, ...</td>\n",
       "      <td>12.939820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         hour  rentals_in_hour  temp  \\\n",
       "0  (2018-10-02 18:00:00, 2018-10-02 19:00:00)               23  28.9   \n",
       "1  (2018-05-25 08:00:00, 2018-05-25 09:00:00)               37  18.9   \n",
       "2  (2018-07-20 08:00:00, 2018-07-20 09:00:00)               32  22.2   \n",
       "3  (2018-07-14 20:00:00, 2018-07-14 21:00:00)               10  32.8   \n",
       "4  (2018-10-18 18:00:00, 2018-10-18 19:00:00)               19  14.4   \n",
       "\n",
       "   rel_humidity  precipitation  snow  wspd  sun_minutes  is_holiday  workday  \\\n",
       "0          57.0            0.0   0.0  20.5          0.0         0.0      1.0   \n",
       "1          81.0            0.0   0.0   7.6          0.0         0.0      1.0   \n",
       "2          76.0            0.0   0.0   0.0          0.0         0.0      1.0   \n",
       "3          31.0            0.0   0.0  22.3          0.0         0.0      1.0   \n",
       "4          36.0            0.0   0.0  24.1          0.0         0.0      1.0   \n",
       "\n",
       "   month  hour_of_day    year  \\\n",
       "0   10.0         18.0  2018.0   \n",
       "1    5.0          8.0  2018.0   \n",
       "2    7.0          8.0  2018.0   \n",
       "3    7.0         20.0  2018.0   \n",
       "4   10.0         18.0  2018.0   \n",
       "\n",
       "                                            features  \\\n",
       "0  [28.9, 57.0, 0.0, 0.0, 20.5, 0.0, 0.0, 1.0, 10...   \n",
       "1  [18.9, 81.0, 0.0, 0.0, 7.6, 0.0, 0.0, 1.0, 5.0...   \n",
       "2  (22.2, 76.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 7.0...   \n",
       "3  [32.8, 31.0, 0.0, 0.0, 22.300000000000004, 0.0...   \n",
       "4  [14.4, 36.0, 0.0, 0.0, 24.1, 0.0, 0.0, 1.0, 10...   \n",
       "\n",
       "                                     features_scaled  prediction  \n",
       "0  [2.9452533173777047, 2.973778726395736, 0.0, 0...   18.439240  \n",
       "1  [1.9261345224373223, 4.225896084878151, 0.0, 0...   27.576794  \n",
       "2  (2.2624437247676483, 3.9650383018609814, 0.0, ...   27.576794  \n",
       "3  [3.3427096474044533, 1.6173182547064529, 0.0, ...    6.295239  \n",
       "4  [1.4675310647141504, 1.8781760377236227, 0.0, ...   12.939820  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outgoings_pd_df = outgoings_test.toPandas()\n",
    "outgoings_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e82432f0-7701-4ac8-b479-b75db680e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoings_pd_df.hour = pd.to_datetime(outgoings_pd_df.hour.str[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f98095f9-ff12-4918-a23f-0127a4ae67dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-02 18:00:00</td>\n",
       "      <td>18.439240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-25 08:00:00</td>\n",
       "      <td>27.576794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-07-20 08:00:00</td>\n",
       "      <td>27.576794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-07-14 20:00:00</td>\n",
       "      <td>6.295239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-18 18:00:00</td>\n",
       "      <td>12.939820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 hour  prediction\n",
       "0 2018-10-02 18:00:00   18.439240\n",
       "1 2018-05-25 08:00:00   27.576794\n",
       "2 2018-07-20 08:00:00   27.576794\n",
       "3 2018-07-14 20:00:00    6.295239\n",
       "4 2018-10-18 18:00:00   12.939820"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outgoings_pd_df = outgoings_pd_df[['hour', 'prediction']]\n",
    "outgoings_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bc885e7-c26d-4e19-bea5-8224a8de28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoings_pd_df.prediction = outgoings_pd_df.prediction.round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1f019e0e-cf41-4c3c-8f68-a6f7efb4190c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-02 18:00:00</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-25 08:00:00</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 hour  prediction\n",
       "0 2018-10-02 18:00:00          18\n",
       "1 2018-05-25 08:00:00          28"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outgoings_pd_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c83f1355-fb58-40f1-9a2e-66a42d9f0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "outgoings_preds = spark.createDataFrame(outgoings_pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87ea6199-00e3-4d83-b7d5-b35f3a1937f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"gs://dscb430-analysis-data-bucket/ml_temp\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d30c0e1-9229-47da-84a2-f1e93b7cddf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 659:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               hour|prediction|\n",
      "+-------------------+----------+\n",
      "|2018-10-02 18:00:00|        18|\n",
      "|2018-05-25 08:00:00|        28|\n",
      "|2018-07-20 08:00:00|        28|\n",
      "|2018-07-14 20:00:00|         6|\n",
      "|2018-10-18 18:00:00|        13|\n",
      "|2018-05-26 23:00:00|         2|\n",
      "|2018-07-11 21:00:00|         5|\n",
      "|2018-02-09 07:00:00|        19|\n",
      "|2019-07-18 06:00:00|        15|\n",
      "|2019-11-18 08:00:00|        26|\n",
      "|2018-07-18 19:00:00|        10|\n",
      "|2019-02-15 18:00:00|        18|\n",
      "|2019-01-10 14:00:00|         3|\n",
      "|2019-04-05 17:00:00|         9|\n",
      "|2018-11-28 07:00:00|        23|\n",
      "|2018-02-27 17:00:00|        14|\n",
      "|2018-02-13 09:00:00|         6|\n",
      "|2019-09-27 15:00:00|         7|\n",
      "|2018-03-31 18:00:00|        13|\n",
      "|2019-07-11 14:00:00|         6|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outgoings_preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "83671f50-3ba6-4bcd-9ab7-598672bb5af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "outgoings_preds.write \\\n",
    "   .format(\"bigquery\") \\\n",
    "   .option(\"temporaryGcsBucket\",\"dscb430-analysis-data-bucket/ml_temp\") \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .save(\"welu420.Bikesharing.unionstation_preds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d436652f-ab3f-4574-b93b-e6ddc2899805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(outgoings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374fa78d-308f-4646-8305-e2d60efe41cb",
   "metadata": {},
   "source": [
    "#### Function for getting and saving predictions for stations\n",
    "A better option would be to create a class for this and divide different operations to methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0beec4e7-df99-41f1-82b0-e88d1552485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "def get_save_predictions_for(station: str, bike_df: SparkDataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        station: (str) Name of the station.\n",
    "        bike_df: (SparkDataFrame) data for all the stations.\n",
    "    Returns: None,\n",
    "                trains the model for the given station and writes back the predictions (incomings and outgoing)\n",
    "                to the table in BigQuery.\n",
    "    Example(s):\n",
    "        (i) # get all unique stations in the dataframe\n",
    "            stations = get_unique_stations(dataframe(stations_col)) # get syntax for pyspark\n",
    "            for station in stations:\n",
    "                get_save_predictions_for(station, pyspark_dataframe)\n",
    "                \n",
    "        (ii) station = 'union_station' # station which exists in the data\n",
    "             get_save_predictions_for(station, pyspark_dataframe)\n",
    "    \"\"\"\n",
    "    global bucket\n",
    "    \n",
    "    # filter to get the dataframe for the given station\n",
    "    station_df = bike_df.filter((bike_df.Start_station == station) | \n",
    "                               (bike_df.End_station == station))\n",
    "    # handling missing values and preparing data\n",
    "    # converting duration in minutes and saving in a new column\n",
    "    station_df = station_df.withColumn('minutes',station_df.Duration/60)\n",
    "    # mean temperature to replace the nan values in temperature column\n",
    "    mean_temp = station_df.select(mean('temp')).collect()[0][0]\n",
    "    \n",
    "    # replace nan values\n",
    "    station_df = station_df.fillna(\n",
    "        {\n",
    "            'temp': mean_temp,\n",
    "            'Holiday': 'No-Holiday',\n",
    "            'rel_humidity': 0,\n",
    "            'precipitation': 0,\n",
    "            'snow': 0,\n",
    "            'sun_minutes': 0,\n",
    "        }\n",
    "    )\n",
    "    # creating new columns: hour_of_day, day_of_week, month and year from timestamp\n",
    "    station_df = station_df.withColumn('hour_of_day', hour('Start_date').alias('hour_of_day'))\n",
    "    station_df = station_df.withColumn('year', year('Start_date').alias('year'))\n",
    "    station_df = station_df.withColumn('month', month('Start_date').alias('month'))\n",
    "    station_df = station_df.withColumn('day_of_week', dayofweek('Start_date').alias('day_of_week'))\n",
    "    # conditional column: 1 if holiday, 0 otherwise\n",
    "    station_df = station_df.withColumn(\n",
    "        'is_holiday',\n",
    "        F.when((F.col(\"Holiday\") == 'No-Holiday'), 0).otherwise(1)\n",
    "    )\n",
    "    # conditional column: 1 if weekend, 0 if day of week between 0-5\n",
    "    station_df = station_df.withColumn(\n",
    "        'workday',\n",
    "        F.when((F.col('day_of_week') == 5) & (F.col('day_of_week') == 6), 0).otherwise(1)\n",
    "    )\n",
    "    # rides ending at this station\n",
    "    # duration is at least 5 minutes\n",
    "    incomings = station_df.filter( \\\n",
    "        (station_df['End_station'] == 'Columbus Circle / Union Station') & \\\n",
    "        (station_df['minutes'] >= 5))\n",
    "\n",
    "    # rides begining from this station\n",
    "    outgoings = station_df.filter( \\\n",
    "        (station_df['Start_station'] == 'Columbus Circle / Union Station') & \\\n",
    "        (station_df['minutes'] >= 5))\n",
    "    # features: temp, rel_humidity, precipitation, snow, wspd, sun_minutes, is_holiday, workday, month, hour_of_day, year\n",
    "    # to predict: rentals in hour\n",
    "    # # grouping by every hour to get the total number of rides for every hour\n",
    "    # # using mean for other features to keep them in the table\n",
    "    # # mean makes no difference as the values for one hour are always same\n",
    "    incomings = (\n",
    "       incomings\n",
    "        .groupBy(window(col(\"Start_date\"), \"1 hour\").alias(\"hour\"))\n",
    "        .agg(\n",
    "            F.count('ride_id').alias('rentals_in_hour'), F.mean('temp').alias('temp'), F.mean('rel_humidity').alias('rel_humidity'),\n",
    "            F.mean('precipitation').alias('precipitation'), F.mean('snow').alias('snow'), F.mean('wspd').alias('wspd'),\n",
    "            F.mean('sun_minutes').alias('sun_minutes'), F.mean('is_holiday').alias('is_holiday'),\n",
    "            F.mean('workday').alias('workday'), F.mean('month').alias('month'), F.mean('hour_of_day').alias('hour_of_day'),\n",
    "            F.mean('year').alias('year')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    outgoings = (\n",
    "       outgoings\n",
    "        .groupBy(window(col(\"Start_date\"), \"1 hour\").alias(\"hour\"))\n",
    "        .agg(\n",
    "            F.count('ride_id').alias('rentals_in_hour'), F.mean('temp').alias('temp'), F.mean('rel_humidity').alias('rel_humidity'),\n",
    "            F.mean('precipitation').alias('precipitation'), F.mean('snow').alias('snow'), F.mean('wspd').alias('wspd'),\n",
    "            F.mean('sun_minutes').alias('sun_minutes'), F.mean('is_holiday').alias('is_holiday'),\n",
    "            F.mean('workday').alias('workday'), F.mean('month').alias('month'), F.mean('hour_of_day').alias('hour_of_day'),\n",
    "            F.mean('year').alias('year')\n",
    "        )\n",
    "    )\n",
    "    # train test split\n",
    "    outgoings_train = outgoings.where(F.col('year') < 2018)\n",
    "    outgoings_test = outgoings.where(F.col('year') > 2017)\n",
    "    incomings_train = incomings.where(F.col('year') < 2018)\n",
    "    incomings_test = incomings.where(F.col('year') > 2017)\n",
    "    # creating our pipeline\n",
    "    # stage 1: create feature vector\n",
    "    vectorising_stage = VectorAssembler(inputCols=['temp', 'rel_humidity', 'precipitation', 'snow', 'wspd',\n",
    "                                                   'sun_minutes', 'is_holiday', 'workday', 'month', 'hour_of_day',\n",
    "                                                   'year'], outputCol='features')\n",
    "    # stage 2: scale feature vector\n",
    "    scaling_stage = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "\n",
    "    # stage 3: estimator\n",
    "    gbr_stage = GBTRegressor(featuresCol='features_scaled', labelCol='rentals_in_hour')\n",
    "\n",
    "    gbr_pipe = Pipeline(stages=[vectorising_stage, scaling_stage, gbr_stage])\n",
    "    \n",
    "    # fit the model on train data\n",
    "    # model/pipe for outgoings\n",
    "    outgoings_model = gbr_pipe.fit(outgoings_train)\n",
    "    # transform train data\n",
    "    outgoings_train = outgoings_model.transform(outgoings_train)\n",
    "    # transform test data and make predictions (running the pipeline on it)\n",
    "    # predictions for outgoings\n",
    "    outgoings_test = outgoings_model.transform(outgoings_test)\n",
    "\n",
    "    # select (prediction, true label) and compute test error\n",
    "    # evaluate the model \n",
    "    evaluator = RegressionEvaluator(labelCol=\"rentals_in_hour\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(outgoings_test)\n",
    "    # fit the model for the incomings\n",
    "    incomings_model = gbr_pipe.fit(incomings_train)\n",
    "    incomings_train = incomings_model.transform(incomings_train)\n",
    "    # transform test data and make predictions (running the pipeline on it)\n",
    "    # predictions for incomings\n",
    "    incomings_test = incomings_model.transform(incomings_test)\n",
    "    # select (prediction, true label) and compute test error\n",
    "    evaluator = RegressionEvaluator(labelCol=\"rentals_in_hour\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "    rmse = evaluator.evaluate(incomings_test)\n",
    "    # need to get the prediction column and clean the table before uploading\n",
    "    outgoings_pd_df = outgoings_test.toPandas()\n",
    "    incomings_pd_df = incomings_test.toPandas()\n",
    "    # datetime col was a tuple\n",
    "    outgoings_pd_df.hour = pd.to_datetime(outgoings_pd_df.hour.str[0])\n",
    "    incomings_pd_df.hour = pd.to_datetime(incomings_pd_df.hour.str[0])\n",
    "    # dropping all other columns\n",
    "    outgoings_pd_df = outgoings_pd_df[['hour', 'prediction']]\n",
    "    incomings_pd_df = incomings_pd_df[['hour', 'prediction']]\n",
    "    outgoings_pd_df.prediction = outgoings_pd_df.prediction.round(0).astype(int)\n",
    "    incomings_pd_df.prediction = incomings_pd_df.prediction.round(0).astype(int)\n",
    "    # writing it back to a Spark DataFrame\n",
    "    outgoings_preds = spark.createDataFrame(outgoings_pd_df)\n",
    "    incomings_preds = spark.createDataFrame(incomings_pd_df)\n",
    "    \n",
    "    outgoings_preds.write \\\n",
    "      .format(\"bigquery\") \\\n",
    "      .option(\"temporaryGcsBucket\",\"dscb430-analysis-data-bucket/ml_temp\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .save(\"welu420.Bikesharing.outgoing_predictions\")\n",
    "    incomings_preds.write \\\n",
    "      .format(\"bigquery\") \\\n",
    "      .option(\"temporaryGcsBucket\",\"dscb430-analysis-data-bucket/ml_temp\") \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .save(\"welu420.Bikesharing.incoming_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea20ba-7efa-4ee7-8772-c4fd55f89a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = [x.Start_station for x in bike_df.select('Start_station').distinct().collect()]\n",
    "#stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68a0cb1-1ecc-494f-ba97-827b71da5fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "bucket = \"gs://dscb430-analysis-data-bucket/ml_temp\"\n",
    "spark.conf.set('temporaryGcsBucket', bucket)\n",
    "\n",
    "for station in stations:\n",
    "    get_save_predictions_for(station, bike_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "79b1884623237b584890ad8843bc31562644d49763ed5551eef9a1f6a1e3f0af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
